"""Citation extraction utilities

Functions to extract reference strings/DOIs from paper analysis JSON produced by the PDF extraction
"""
from typing import Dict, List, Optional
import re
import logging

logger = logging.getLogger(__name__)

DOI_RE = re.compile(r"10\.\d{4,9}/[-._;()/:A-Za-z0-9]+")
BRACKET_REF_RE = re.compile(r"^\s*\[?\d+\]?\s*(.+)")


def extract_references_from_paper(paper_data: Dict) -> List[Dict]:
    """Extract references from a paper analysis dict.

    Looks for:
      - explicit 'references' key (list or string)
      - 'sections' -> 'references' content

    Returns list of references as dicts: {'raw': str, 'doi': Optional[str], 'title': Optional[str]}
    """
    refs_raw = []

    # 1) direct references field
    if 'references' in paper_data and paper_data['references']:
        refs = paper_data['references']
        if isinstance(refs, list):
            refs_raw.extend(refs)
        elif isinstance(refs, str):
            refs_raw.extend([r.strip() for r in refs.split('\n') if r.strip()])

    # 2) sections -> references
    sections = paper_data.get('sections', {})
    if isinstance(sections, dict):
        # common key names
        for key in ['references', 'reference', 'bibliography', 'references_and_notes']:
            sec = sections.get(key)
            if sec:
                if isinstance(sec, dict):
                    content = sec.get('content') or ''
                else:
                    content = sec
                refs_raw.extend([r.strip() for r in content.split('\n') if r.strip()])

    # 3) fallback: metadata 'references_text'
    meta = paper_data.get('metadata', {})
    if isinstance(meta, dict) and meta.get('references_text'):
        refs_raw.extend([r.strip() for r in meta.get('references_text', '').split('\n') if r.strip()])

    results = []
    for r in refs_raw:
        if not r:
            continue
        doi_match = DOI_RE.search(r)
        doi = doi_match.group(0) if doi_match else None
        # try to extract title portion by stripping leading numbers
        title = None
        m = BRACKET_REF_RE.match(r)
        if m:
            title = m.group(1).strip()
        else:
            title = r
        results.append({'raw': r, 'doi': doi, 'title': title})

    # dedupe on raw
    seen = set()
    deduped = []
    for item in results:
        key = (item.get('doi') or item.get('title') or item.get('raw'))
        if key in seen:
            continue
        seen.add(key)
        deduped.append(item)

    logger.info(f"Extracted {len(deduped)} references")
    return deduped


def get_paper_identifier(paper_data: Dict, fallback_filename: Optional[str] = None) -> str:
    """Return a stable identifier for a paper.

    Prefer DOI in metadata, else title, else filename.
    """
    meta = paper_data.get('metadata', {}) or {}
    doi = meta.get('doi') or meta.get('DOI')
    if doi:
        return doi
    title = meta.get('title') or paper_data.get('title')
    if title:
        return title.strip()
    if fallback_filename:
        return fallback_filename
    return paper_data.get('id') or paper_data.get('file_name') or 'unknown'


def collect_analysis_jsons_from_dir(folder: str) -> Dict[str, Dict]:
    """Load all *.json analysis files from a folder and return mapping id->paper_data.

    The function expects files generated by the PDF extraction step (e.g. shang2024_analysis.json).
    """
    import os
    import json

    papers = {}
    for fname in os.listdir(folder):
        if not fname.lower().endswith('.json'):
            continue
        path = os.path.join(folder, fname)
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            pid = get_paper_identifier(data, fallback_filename=os.path.splitext(fname)[0])
            # store filename for later
            data['_source_file'] = fname
            papers[pid] = data
        except Exception as e:
            logger.warning(f"Failed to load {path}: {e}")
    return papers
